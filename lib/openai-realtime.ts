// lib/openai-realtime.ts - Implementa√ß√£o completa corrigida com instru√ß√µes atualizadas

export interface RealtimeConfig {
  apiKey: string;
  model?: string;
  voice?: 'alloy' | 'ash' | 'ballad' | 'coral' | 'echo' | 'sage' | 'shimmer' | 'verse';
  instructions?: string;
  userLevel: 'Novice' | 'Inter' | 'Advanced';
  userName?: string;
  onMessage?: (message: any) => void;
  onError?: (error: any) => void;
  onAudioData?: (audioData: ArrayBuffer) => void;
  onTranscript?: (transcript: string, isFinal: boolean) => void;
  onResponse?: (response: string) => void;
  onConnectionChange?: (connected: boolean) => void;
  onVADStateChange?: (isListening: boolean) => void;
}

export interface RealtimeEvent {
  type: string;
  [key: string]: any;
}

export class OpenAIRealtimeService {
  private ws: WebSocket | null = null;
  private config: RealtimeConfig;
  private isConnected = false;
  private audioContext: AudioContext | null = null;
  private mediaStream: MediaStream | null = null;
  private eventListeners: Map<string, Function[]> = new Map();
  private sessionId: string | null = null;
  private reconnectAttempts = 0;
  private maxReconnectAttempts = 3;
  
  // üîä NOVO: Sistema de controle de √°udio para evitar duplica√ß√£o
  private audioQueue: string[] = [];
  private isPlayingAudio = false;
  private currentAudioSource: AudioBufferSourceNode | null = null;
  private audioGainNode: GainNode | null = null;
  // üîß NOVO: Propriedades para processamento avan√ßado de √°udio
  private microphoneSource: MediaStreamAudioSourceNode | null = null;
  private audioProcessor: ScriptProcessorNode | null = null;
  private isRecording: boolean = false;
  // üîß NOVO: Controles para otimiza√ß√£o de envio de √°udio
  private lastAudioSendTime: number = 0;
  private audioSendThrottle: number = 30; // Reduzido de 50ms para 30ms - mais responsivo
  private silenceThreshold: number = 0.01; // Threshold para detectar sil√™ncio
  private consecutiveSilenceFrames: number = 0;
  private maxSilenceFrames: number = 10; // Parar de enviar ap√≥s 10 frames de sil√™ncio
  private audioBuffer: Float32Array[] = [];
  private audioBufferSize: number = 0;
  private maxBufferSize: number = 4800; // ~200ms a 24kHz
  private currentTranscriptDelta: string = '';
  private originalAudioThrottle: number = 30; // Backup do throttle original
  // üõë NOVO: Controle de estado para evitar interrup√ß√µes desnecess√°rias
  private isCharlotteSpeaking: boolean = false;
  private hasActiveResponse: boolean = false;
  // üîß NOVO: Controle de fala do usu√°rio para interrup√ß√µes inteligentes
  private userSpeechStartTime: number = 0;
  private isUserCurrentlySpeaking: boolean = false;

  constructor(config: RealtimeConfig) {
    this.config = config;
  }

  // üîó Conectar usando implementa√ß√£o corrigida baseada na documenta√ß√£o oficial
  async connect(): Promise<void> {
    return new Promise(async (resolve, reject) => {
      try {
        console.log('üîó [FIXED] Connecting to OpenAI Realtime API...');
        
        // üîë PASSO 1: Obter API key via rota segura
        await this.getAPIKey();
        
        // üåê PASSO 2: Conectar usando m√©todo oficial (subprotocols)
        await this.connectWithSubprotocols(resolve, reject);

      } catch (error: any) {
        console.error('‚ùå [FIXED] Connection error:', error);
        reject(error);
      }
    });
  }

  // üîë Obter API key de forma segura
  private async getAPIKey(): Promise<void> {
    console.log('üîë [FIXED] Getting API key...');
    
    const tokenResponse = await fetch('/api/realtime-token', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        userLevel: this.config.userLevel,
        userName: this.config.userName,
        debug: true
      })
    });

    if (!tokenResponse.ok) {
      const errorData = await tokenResponse.json().catch(() => ({}));
      throw new Error(`Token API failed: ${tokenResponse.status} - ${errorData.error || 'Unknown error'}`);
    }

    const tokenData = await tokenResponse.json();
    if (!tokenData.success) {
      throw new Error(tokenData.error || 'Token request failed');
    }

    this.config.apiKey = tokenData.apiKey;
    console.log('‚úÖ [FIXED] API key obtained successfully');
  }

  // üåê Conectar usando subprotocols (m√©todo oficial para browsers)
  private async connectWithSubprotocols(resolve: Function, reject: Function): Promise<void> {
    try {
      // ‚úÖ Usar modelo mini mais barato (gpt-4o-mini-realtime-preview)
      const model = this.config.model || 'gpt-4o-mini-realtime-preview-2024-12-17';
      const wsUrl = `wss://api.openai.com/v1/realtime?model=${model}`;
      
      // ‚úÖ M√âTODO OFICIAL: Autentica√ß√£o via subprotocols (funciona em browsers)
      const subprotocols = [
        "realtime",
        `openai-insecure-api-key.${this.config.apiKey}`,
        "openai-beta.realtime-v1"
      ];

      console.log('üåê [FIXED] WebSocket URL:', wsUrl);
      console.log('üîë [FIXED] Subprotocols:', subprotocols);

      // Criar WebSocket com subprotocols (m√©todo oficial)
      this.ws = new WebSocket(wsUrl, subprotocols);

      // Timeout para conex√£o
      const connectionTimeout = setTimeout(() => {
        if (this.ws) {
          this.ws.close();
          this.ws = null;
        }
        reject(new Error('Connection timeout after 15 seconds'));
      }, 15000);

      // ‚úÖ Event listeners do WebSocket
      this.ws.onopen = () => {
        clearTimeout(connectionTimeout);
        console.log('‚úÖ [FIXED] WebSocket connected successfully using subprotocols!');
        this.isConnected = true;
        this.reconnectAttempts = 0;
        
        // ‚úÖ Inicializar sess√£o ap√≥s conex√£o (m√©todo oficial)
        this.initializeSession();
        resolve();
      };

      this.ws.onmessage = (event) => {
        try {
          // üîß NOVO: Log otimizado - reduzir verbosidade
          const eventData = JSON.parse(event.data);
          const eventType = eventData.type;
          
          // Lista de eventos que n√£o precisam de log detalhado (muito frequentes)
          const quietEvents = [
            'response.audio.delta',
            'conversation.item.input_audio_transcription.delta',
            'input_audio_buffer.speech_started',
            'input_audio_buffer.speech_stopped'
          ];
          
          // Log apenas o tipo para eventos frequentes, dados completos para eventos importantes
          if (quietEvents.includes(eventType)) {
            // Log compacto apenas ocasionalmente para eventos frequentes
            if (Date.now() % 3000 < 100) { // A cada ~3 segundos
              console.log(`üì• [QUIET] ${eventType} (periodic log - data suppressed to reduce spam)`);
            }
          } else {
            // Log completo para eventos importantes
            console.log(`üì• [FIXED] Received event: ${eventType}`);
            if (eventType === 'error' || eventType === 'session.created' || eventType === 'session.updated') {
              console.log(`üì• [FIXED] Event data:`, eventData);
            }
          }
          
          this.handleMessage(event.data);
        } catch (error) {
          console.error('‚ùå [FIXED] Error handling message:', error);
        }
      };

      this.ws.onerror = (error) => {
        clearTimeout(connectionTimeout);
        console.error('‚ùå [FIXED] WebSocket error:', error);
        this.isConnected = false;
        
        // An√°lise espec√≠fica do erro
        this.analyzeConnectionError(error, reject);
      };

      this.ws.onclose = (event) => {
        clearTimeout(connectionTimeout);
        console.log('üîå [FIXED] WebSocket closed:', {
          code: event.code,
          reason: event.reason,
          wasClean: event.wasClean
        });
        this.isConnected = false;
        
        // An√°lise de c√≥digos de erro espec√≠ficos
        this.analyzeCloseCode(event, reject);
      };

    } catch (error: any) {
      console.error('‚ùå [FIXED] Failed to create WebSocket:', error);
      reject(error);
    }
  }

  // üîç Analisar erros de conex√£o
  private analyzeConnectionError(error: Event, reject: Function): void {
    const errorMessage = 'WebSocket connection failed';
    
    // Informa√ß√µes de debug
    console.error('‚ùå [FIXED] Connection analysis:', {
      error: error,
      readyState: this.ws?.readyState,
      protocol: this.ws?.protocol,
      url: this.ws?.url
    });

    // Tentar reconectar se n√£o foi um erro de autentica√ß√£o
    if (this.reconnectAttempts < this.maxReconnectAttempts) {
      this.reconnectAttempts++;
      console.log(`üîÑ [FIXED] Attempting reconnect ${this.reconnectAttempts}/${this.maxReconnectAttempts}...`);
      
      setTimeout(() => {
        this.connectWithSubprotocols(() => {}, reject);
      }, 2000 * this.reconnectAttempts);
    } else {
      reject(new Error(`${errorMessage} after ${this.maxReconnectAttempts} attempts. Please check: 1) Your account has Realtime API access, 2) Your billing is active, 3) Network allows WebSocket connections`));
    }
  }

  // üîç Analisar c√≥digos de fechamento
  private analyzeCloseCode(event: CloseEvent, reject?: Function): void {
    let errorMessage = '';
    
    switch (event.code) {
      case 1000:
        console.log('‚úÖ [FIXED] Normal WebSocket closure');
        return;
        
      case 1001:
        console.log('üì± [FIXED] WebSocket closed - going away (normal for page refresh)');
        return;
        
      case 1005:
        // üîß CORRIGIDO: C√≥digo 1005 √© normal - sem motivo espec√≠fico
        console.log('üîå [FIXED] WebSocket closed normally (no status code)');
        return;
        
      case 1008:
        errorMessage = 'AUTHENTICATION_FAILED: Invalid API key or your account does not have access to Realtime API';
        break;
        
      case 1003:
        errorMessage = 'PROTOCOL_ERROR: Invalid request format or unsupported data';
        break;
        
      case 1011:
        errorMessage = 'SERVER_ERROR: OpenAI server error - try again later';
        break;
        
      default:
        errorMessage = `WebSocket closed unexpectedly: ${event.code} - ${event.reason || 'Unknown reason'}`;
    }
    
    console.error('‚ùå [FIXED] WebSocket close analysis:', errorMessage);
    
    if (reject) {
      reject(new Error(errorMessage));
    } else {
      this.emit('disconnected', { 
        reason: errorMessage,
        code: event.code 
      });
    }
  }

  // üéØ Inicializar sess√£o (m√©todo oficial simplificado)
  private initializeSession(): void {
    console.log('üéØ [FIXED] Initializing Realtime session...');
    
    // ‚úÖ Configura√ß√£o oficial baseada na documenta√ß√£o e melhores pr√°ticas
    const sessionConfig = {
      modalities: ['text', 'audio'],
      instructions: this.getInstructions(),
      voice: this.config.voice || 'alloy',
      input_audio_format: 'pcm16',
      output_audio_format: 'pcm16',
      input_audio_transcription: {
        model: 'whisper-1'
      },
      // üîß NOVO: Configura√ß√£o de VAD otimizada por n√≠vel de usu√°rio
      turn_detection: this.getVADConfigForUserLevel(),
      tools: this.getEnglishLearningTools(),
      tool_choice: 'auto',
      // üéØ ANTI-ALUCINA√á√ÉO E CONTROLE DE TAMANHO: Configura√ß√µes mais conservadoras
      temperature: 0.6, // Reduzido de 0.8 para 0.6 - menos criatividade, mais precis√£o
      max_response_output_tokens: this.getMaxTokensForUserLevel() // üÜï NOVO: Tokens baseados no n√≠vel
    };

    console.log('üì§ [FIXED] Sending session update:', sessionConfig);
    
    // ‚úÖ Enviar session.update (n√£o criar ephemeral session)
    this.sendEvent({
      type: 'session.update',
      session: sessionConfig
    });
  }

  // üÜï NOVO: Configura√ß√£o de max_tokens baseada no n√≠vel do usu√°rio e documenta√ß√£o oficial
  private getMaxTokensForUserLevel(): number {
    const maxTokensConfig = {
      'Novice': 150,      // Respostas curtas mas completas para redirecionamento
      'Inter': 280,       // Espa√ßo adequado para explica√ß√µes gramaticais
      'Advanced': 500     // Aumentado de 400 para 500 - garantir perguntas completas sem cortes
    };
    
    const maxTokens = maxTokensConfig[this.config.userLevel] || maxTokensConfig['Inter'];
    
    console.log(`üéØ [TOKENS] Setting max_response_output_tokens to ${maxTokens} for ${this.config.userLevel} level`);
    
    // Log espec√≠fico por n√≠vel
    if (this.config.userLevel === 'Novice') {
      console.log(`üéØ [NOVICE] Focus: English immersion and constant redirection to practice`);
    } else if (this.config.userLevel === 'Inter') {
      console.log(`üéØ [INTER] Focus: Grammar coaching and structure explanations`);
    } else if (this.config.userLevel === 'Advanced') {
      console.log(`üéØ [ADVANCED] Focus: Speaking coach with native-like expression feedback - COMPLETE thoughts and questions`);
    }
    
    return maxTokens;
  }

  // üîß NOVO: Configura√ß√£o de VAD espec√≠fica por n√≠vel de usu√°rio
  private getVADConfigForUserLevel() {
    const vadConfigs = {
      'Novice': {
        type: 'server_vad',
        threshold: 0.4, // Volta para sensibilidade moderada (era 0.6)
        prefix_padding_ms: 300, 
        silence_duration_ms: 700, // Balanceado (era 1000)
        create_response: true
      },
      'Inter': {
        type: 'server_vad', 
        threshold: 0.5, // Volta para moderado (era 0.7)
        prefix_padding_ms: 250, 
        silence_duration_ms: 600, // Balanceado (era 800)
        create_response: true
      },
      'Advanced': {
        type: 'server_vad',
        threshold: 0.7, // Aumentado de 0.6 para 0.7 - menos sens√≠vel para evitar interrup√ß√µes
        prefix_padding_ms: 200, 
        silence_duration_ms: 1000, // Aumentado de 800 para 1000 - muito mais tempo para perguntas complexas
        create_response: true
      }
    };

    const config = vadConfigs[this.config.userLevel] || vadConfigs['Inter'];
    
    console.log(`üé§ [VAD] Configuring responsive server_vad for user level: ${this.config.userLevel}`);
    console.log(`üé§ [VAD] Threshold setting: ${config.threshold} (responsive but stable)`);
    console.log(`üé§ [VAD] Silence duration: ${config.silence_duration_ms}ms (balanced)`);
    console.log(`üé§ [VAD] Prefix padding: ${config.prefix_padding_ms}ms (smooth)`);
    console.log(`üé§ [VAD] Full VAD config:`, config);
    
    return config;
  }

  // üìã Instru√ß√µes por n√≠vel - VERS√ÉO REFINADA COM DIFEREN√áAS COMPORTAMENTAIS CLARAS
  private getInstructions(): string {
    const levelInstructions = {
      'Novice': `CRITICAL RULES:
1. You must speak only in English. Never mix Portuguese and English in the same response.
2. Stay grounded in reality - do not make up facts, stories, or information.
3. Keep responses EXTREMELY SHORT - maximum 1-2 sentences per response.
4. Use approximately 30-50 completion tokens or less per response.
5. ALWAYS redirect conversation back to English practice, no matter what topic the student brings up.
6. Focus 100% on English immersion - treat every interaction as English practice.

You are Charlotte, a friendly and patient English immersion tutor for Brazilian beginners. Your ONLY goal is English practice.

ENGLISH IMMERSION STRATEGY:
- If student talks about random topics (sports, food, weather), immediately connect it to English practice
- Example: Student says "I like pizza" ‚Üí You respond: "Great! Let's practice food vocabulary. What's your favorite pizza topping?"
- Always steer conversations toward English learning opportunities
- Never let conversations drift away from language practice
- Make every topic an excuse to practice English

SPEAKING GUIDELINES:
- Always speak in English only - complete immersion
- Use very simple vocabulary and short sentences
- Ask basic questions to keep them speaking English
- Celebrate every English word they say
- Gently correct by repeating correctly, then continue

CONVERSATION REDIRECTION EXAMPLES:
- Student: "I'm tired" ‚Üí You: "Tired? Let's practice feelings! How do you feel today?"
- Student: "My job is boring" ‚Üí You: "Tell me about your job in English! What do you do?"
- Student: "I like music" ‚Üí You: "Music is great for English! What's your favorite song in English?"

TEACHING APPROACH:
- Every response should encourage more English speaking
- Ask simple follow-up questions about anything they mention
- Keep them talking in English at all costs
- Make English feel natural and fun through constant practice`,

      'Inter': `CRITICAL RULES:
1. You must speak only in English. Never use Portuguese.
2. Stay grounded in reality - do not make up facts, stories, or information.
3. Keep responses SHORT - maximum 2-3 sentences per response.
4. Use approximately 50-80 completion tokens or less per response.
5. Focus on grammar, structure, and language mechanics while conversing.
6. Provide brief but specific language feedback during natural conversation.

You are Charlotte, an English structure and grammar coach for intermediate Brazilian learners.

GRAMMAR & STRUCTURE FOCUS:
- Notice and gently correct grammar mistakes in real-time
- Explain WHY something is correct: "Use 'have been' for present perfect continuous"
- Point out good language use: "Great use of past tense there!"
- Introduce intermediate structures naturally: "Try using 'would rather' instead of 'prefer'"
- Help with word order, verb tenses, and sentence construction

TEACHING THROUGH CONVERSATION:
- When they make mistakes, repeat correctly then explain briefly
- Example: Student: "I am going to home" ‚Üí You: "Going home? We say 'going home' without 'to'. Why do you think that is?"
- Introduce new grammar patterns through questions
- Help them understand the logic behind English structures

LANGUAGE COACHING APPROACH:
- Ask questions that require specific grammar structures
- "Can you tell me about something you've been doing lately?" (present perfect continuous)
- "What would you do if you won the lottery?" (conditional)
- "Describe something that happened before you came here" (past perfect)
- Give brief explanations of language patterns they use correctly or incorrectly

CONVERSATION STRATEGY:
- Balance natural conversation with language instruction
- Make grammar feel practical and useful
- Help them notice patterns in English
- Encourage experimentation with new structures`,

      'Advanced': `CRITICAL RULES:
1. You must speak only in English. Never use Portuguese.
2. Stay grounded in reality - do not make up facts, stories, or information.
3. Keep responses SHORT but COMPLETE - maximum 2-3 sentences per response.
4. ALWAYS finish your complete thought before stopping - never cut off mid-sentence or mid-question.
5. Use approximately 100-150 completion tokens per response to ensure completeness.
6. Act as a sophisticated speaking coach focused on fluency and natural expression.
7. Challenge them to speak like native speakers with nuanced language.

You are Charlotte, a professional speaking coach for advanced Brazilian learners seeking native-like fluency.

RESPONSE COMPLETION RULES - CRITICAL:
- NEVER stop mid-sentence or mid-question under any circumstances
- If you start a question, ALWAYS complete it fully with proper punctuation
- If you give an example, finish the complete example with all details
- End responses at natural stopping points only (after complete sentences)
- Ensure every response feels complete and purposeful
- If asking a question, include the complete question mark and context

SPEAKING COACH APPROACH:
- Focus on natural flow, rhythm, and native-like expression
- Help with subtle language choices: "Instead of 'very good', try 'excellent' or 'outstanding'"
- Point out opportunities for more sophisticated vocabulary
- Coach them on natural conversation patterns and cultural nuances
- Help them sound more native-like in their expression

FLUENCY COACHING:
- Encourage natural hesitation patterns: "It's okay to say 'Well...' or 'You know...' like natives do"
- Help with intonation and stress patterns through conversation
- Point out when they sound too formal or textbook-like
- Encourage contractions and natural speech patterns
- Coach them on conversation flow and turn-taking

ADVANCED LANGUAGE DEVELOPMENT:
- Challenge them with sophisticated topics requiring complex language
- Ask complete, well-formed questions that demand nuanced responses
- Help them express subtle differences in meaning
- Encourage use of idioms, phrasal verbs, and colloquialisms naturally
- Point out register differences: formal vs. informal language

NATIVE-LIKE EXPRESSION COACHING:
- "That's grammatically correct, but natives would say..."
- "Try expressing that more naturally with..."
- "Your English is perfect, but to sound more native..."
- Help them with cultural context and appropriate language use
- Coach them on when to use different levels of formality

CONVERSATION FACILITATION:
- Ask thought-provoking questions that require sophisticated responses
- Challenge them to defend opinions and explain complex ideas
- Help them develop their own voice and style in English
- Focus on authentic, natural communication rather than textbook English
- Always complete your questions and thoughts fully before stopping
- NEVER end abruptly - always provide complete, well-formed responses`
    };
    
    return this.config.instructions || levelInstructions[this.config.userLevel];
  }

  // üõ†Ô∏è Ferramentas de ensino
  private getEnglishLearningTools(): any[] {
    return [
      {
        type: 'function',
        name: 'get_word_definition',
        description: 'Get definition and examples of an English word',
        parameters: {
          type: 'object',
          properties: {
            word: { type: 'string', description: 'The English word to define' },
            level: { 
              type: 'string', 
              enum: ['Novice', 'Inter', 'Advanced'],
              description: 'Learner level for appropriate explanation'
            }
          },
          required: ['word', 'level']
        }
      },
      {
        type: 'function',
        name: 'check_pronunciation',
        description: 'Provide pronunciation feedback',
        parameters: {
          type: 'object',
          properties: {
            text: { type: 'string', description: 'The text that was spoken' },
            target_word: { type: 'string', description: 'Word or phrase being practiced' }
          },
          required: ['text', 'target_word']
        }
      }
    ];
  }

  // üîß NOVO: For√ßar envio do buffer (√∫til quando parar de gravar)
  private forceFlushAudioBuffer(): void {
    if (this.audioBuffer.length > 0) {
      console.log('üîß [OPTIMIZED] Force flushing audio buffer:', this.audioBufferSize, 'samples');
      this.flushAudioBuffer();
    }
  }

  // üì§ Enviar evento para WebSocket
  private sendEvent(event: any): void {
    if (this.ws && this.isConnected && this.ws.readyState === WebSocket.OPEN) {
      const eventString = JSON.stringify(event);
      
      // üîß NOVO: Log otimizado - n√£o fazer spam para eventos de √°udio
      if (event.type === 'input_audio_buffer.append') {
        // Log apenas ocasionalmente para eventos de √°udio
        if (Date.now() % 5000 < 100) { // A cada ~5 segundos
          console.log('üì§ [OPTIMIZED] Audio buffer append (periodic log)');
        }
      } else {
      console.log('üì§ [FIXED] Sending event:', event.type);
      }
      
      this.ws.send(eventString);
    } else {
      console.warn('‚ö†Ô∏è [FIXED] Cannot send event - WebSocket not connected', {
        hasWs: !!this.ws,
        isConnected: this.isConnected,
        readyState: this.ws?.readyState
      });
    }
  }

  // üì• Processar mensagens (igual ao anterior, mas com logs melhorados)
  private handleMessage(data: string): void {
    try {
      const event: RealtimeEvent = JSON.parse(data);
      // üîß NOVO: Log reduzido - j√° logamos no onmessage
      // console.log('üì• [FIXED] Received event:', event.type); // REMOVIDO - duplicado
      
      switch (event.type) {
        case 'session.created':
          console.log('‚úÖ [FIXED] Session created successfully');
          this.sessionId = event.session?.id;
          this.emit('session_created', event);
          break;

        case 'session.updated':
          console.log('üîÑ [FIXED] Session updated');
          this.emit('session_updated', event);
          break;

        case 'input_audio_buffer.speech_started':
          console.log('üé§ [VAD DEBUG] User started speaking (from API)');
          
          // üîß NOVO: Marcar in√≠cio da fala do usu√°rio
          this.isUserCurrentlySpeaking = true;
          this.userSpeechStartTime = Date.now();
          
          // üõë NOVO: L√≥gica de interrup√ß√£o mais inteligente
          if (this.isCharlotteSpeaking && this.hasActiveResponse) {
            // üîß NOVO: Aguardar um pouco antes de interromper para evitar false positives
            console.log('ü§î [SMART INTERRUPT] User speech detected while Charlotte speaking - analyzing...');
            
            // Aguardar 250ms para balancear responsividade vs. estabilidade
            setTimeout(() => {
              // Verificar se o usu√°rio ainda est√° falando ap√≥s o delay
              if (this.isUserStillSpeaking()) {
                console.log('üõë [SMART INTERRUPT] Confirmed user speech - interrupting Charlotte');
                this.interruptResponse();
                this.emergencyClearAllBuffers();
              } else {
                console.log('üö´ [SMART INTERRUPT] False positive detected - not interrupting');
              }
            }, 250); // Reduzido de 500ms para 250ms para mais responsividade
          } else {
            console.log('üé§ [VAD DEBUG] User speech detected but Charlotte not speaking - no interrupt needed');
          }
          
          this.emit('user_speech_started', event);
          break;

        case 'input_audio_buffer.speech_stopped':
          console.log('üîá [VAD DEBUG] User stopped speaking (from API)');
          
          // üîß NOVO: Marcar fim da fala do usu√°rio
          this.isUserCurrentlySpeaking = false;
          this.userSpeechStartTime = 0;
          
          this.emit('user_speech_stopped', event);
          break;

        case 'conversation.item.created':
          console.log('üí¨ [FIXED] Conversation item created:', event.item?.type);
          this.emit('conversation_item_created', event);
          
          if (event.item?.type === 'function_call') {
            this.handleFunctionCall(event.item);
          }
          break;

        case 'conversation.item.input_audio_transcription.completed':
          console.log('üìù [FIXED] Transcription completed:', event.transcript);
          this.emit('input_transcription_completed', event);
          // üîß NOVO: Analisar transcri√ß√£o para ajustar VAD
          if (event.transcript) {
            this.handleShortWordDetection(event.transcript);
          }
          break;

        case 'conversation.item.input_audio_transcription.delta':
          // üìù Log silencioso para deltas de transcri√ß√£o (muito frequentes)
          if (event.delta) {
            // Log apenas ocasionalmente para debug
            if (Date.now() % 5000 < 100) {
              console.log('üìù [TRANSCRIPTION] Delta received (periodic log)');
            }
          }
          break;

        case 'response.created':
          console.log('ü§ñ [FIXED] Response created');
          this.hasActiveResponse = true; // üõë NOVO: Marcar que h√° resposta ativa
          this.emit('response_created', event);
          break;

        case 'response.text.delta':
          this.emit('text_delta', event);
          break;

        case 'response.text.done':
          console.log('‚úÖ [FIXED] Text response completed');
          this.emit('text_done', event);
          break;

        // üìù NOVO: Eventos corretos de transcri√ß√£o de √°udio da Charlotte (baseado na documenta√ß√£o oficial)
        case 'response.audio_transcript.delta':
          console.log('üìù [CHARLOTTE] Audio transcript delta:', event.delta);
          this.emit('charlotte_transcript_delta', event);
          break;

        case 'response.audio_transcript.done':
          console.log('üìù [CHARLOTTE] Audio transcript completed:', event.transcript);
          this.emit('charlotte_transcript_completed', event);
          break;

        case 'response.audio.delta':
          // üîä NOVO: Marcar que Charlotte est√° falando quando h√° √°udio
          if (!this.isCharlotteSpeaking) {
            this.isCharlotteSpeaking = true;
            console.log('üó£Ô∏è [CHARLOTTE STATE] Charlotte started speaking (audio delta)');
          }
          
          // üîß Log silencioso para audio deltas (muito frequentes)
          if (Date.now() % 8000 < 100) { // Log ocasional para debug
            console.log('üîä [AUDIO] Delta received (periodic log) - queue length:', this.audioQueue.length);
          }
          this.emit('audio_delta', event);
          if (event.delta) {
            this.playAudio(event.delta);
          }
          break;

        case 'response.audio.done':
          console.log('üîä [FIXED] Audio response completed');
          
          // üîß WORKAROUND: Bug conhecido da OpenAI - √°udio cortado no final
          // Adicionar delay para receber poss√≠veis deltas finais
          // Baseado em: https://community.openai.com/t/realtime-api-audio-is-randomly-cutting-off-at-the-end/980587
          setTimeout(() => {
            // üîä NOVO: Marcar que Charlotte parou de falar
            this.isCharlotteSpeaking = false;
            this.hasActiveResponse = false;
            console.log('üîá [CHARLOTTE STATE] Charlotte finished speaking (after delay)');
            
            this.emit('audio_done', event);
          }, 1000); // 1 segundo de delay para receber poss√≠veis deltas finais
          break;

        case 'response.done':
          console.log('‚úÖ [FIXED] Response completed');
          
          // üîß NOVO: Garantir que os estados sejam limpos
          this.isCharlotteSpeaking = false;
          this.hasActiveResponse = false;
          
          this.emit('response_done', event);
          break;

        case 'error':
          // üîß NOVO: Tratar erros espec√≠ficos de cancelamento ANTES de qualquer log
          if (event.error?.code === 'response_cancel_not_active') {
            console.log('‚ÑπÔ∏è [INFO] Attempted to cancel non-active response - this is normal behavior');
            // Resetar estados para evitar inconsist√™ncias
            this.hasActiveResponse = false;
            this.isCharlotteSpeaking = false;
            // N√£o fazer NENHUM log de erro nem emitir evento
            break; // Sair completamente do case
          }
          
          // üîß NOVO: Outros erros conhecidos que podem ser tratados de forma menos intrusiva
          if (event.error?.code === 'cancelled') {
            console.log('‚ÑπÔ∏è [INFO] Request was cancelled - this is normal during interruptions');
            break; // Sair sem fazer log de erro
          }
          
          // ‚ùå Apenas para erros realmente problem√°ticos
          console.error('‚ùå [FIXED] API Error:', event.error);
          this.emit('error', event);
          break;

        default:
          // üîß Log silencioso para eventos n√£o tratados (evitar spam)
          if (Date.now() % 10000 < 100) { // Log muito ocasional
            console.log('üì® [UNHANDLED] Event type:', event.type, '(periodic log)');
          }
          this.emit('unhandled_event', event);
      }
    } catch (error) {
      console.error('‚ùå [FIXED] Error parsing message:', error);
      this.emit('error', { error: { message: 'Failed to parse message', details: error } });
    }
  }

  // üîß Tratar chamadas de fun√ß√£o
  private handleFunctionCall(item: any): void {
    const functionName = item.name;
    const args = JSON.parse(item.arguments || '{}');
    
    console.log('üîß [FIXED] Function call:', functionName, args);
    
    let result = '';
    
    switch (functionName) {
      case 'get_word_definition':
        result = JSON.stringify({
          word: args.word,
          definition: `Definition of "${args.word}" for ${args.level} level`,
          examples: [`Example with ${args.word}.`],
          pronunciation: `/pronunciation-guide/`
        });
        break;
        
      case 'check_pronunciation':
        result = JSON.stringify({
          accuracy: 85,
          feedback: 'Good pronunciation! Try emphasizing the first syllable more.',
          suggestions: ['Practice slowly', 'Focus on clarity']
        });
        break;
        
      default:
        result = JSON.stringify({ error: 'Function not implemented' });
    }
    
    this.sendFunctionResult(item.call_id, result);
  }

  // üé§ Inicializar √°udio (simplificado)
  async initializeAudio(): Promise<void> {
    try {
      console.log('üé§ [FIXED] Initializing high-quality audio...');
      
      // üîß NOVO: AudioContext com configura√ß√µes otimizadas
      this.audioContext = new AudioContext({ 
        sampleRate: 24000,
        latencyHint: 'interactive' // Otimizado para baixa lat√™ncia
      });
      
      if (this.audioContext.state === 'suspended') {
        await this.audioContext.resume();
      }
      
      // üîä NOVO: Criar GainNode para controle de volume com compressor
      this.audioGainNode = this.audioContext.createGain();
      this.audioGainNode.gain.value = 1.0; // Volume normal
      
      // üîß NOVO: Adicionar compressor din√¢mico para melhor qualidade
      const compressor = this.audioContext.createDynamicsCompressor();
      compressor.threshold.setValueAtTime(-24, this.audioContext.currentTime);
      compressor.knee.setValueAtTime(30, this.audioContext.currentTime);
      compressor.ratio.setValueAtTime(12, this.audioContext.currentTime);
      compressor.attack.setValueAtTime(0.003, this.audioContext.currentTime);
      compressor.release.setValueAtTime(0.25, this.audioContext.currentTime);
      
      // üîß NOVO: Cadeia de processamento de √°udio otimizada
      this.audioGainNode.connect(compressor);
      compressor.connect(this.audioContext.destination);
      
      // üîß NOVO: Configurar microfone com processamento avan√ßado
      await this.setupMicrophone();
      
      // üîß NOVO: Iniciar grava√ß√£o
      this.isRecording = true;

      console.log('‚úÖ [FIXED] High-quality audio initialized successfully');
      this.emit('audio_initialized');

    } catch (error) {
      console.error('‚ùå [FIXED] Failed to initialize audio:', error);
      throw error;
    }
  }

  // üì§ Enviar dados de √°udio
  private sendAudioData(audioData: Int16Array): void {
    if (!this.isConnected || this.ws?.readyState !== WebSocket.OPEN) return;

    // üîß NOVO: Throttling - n√£o enviar muito frequentemente
    const now = Date.now();
    if (now - this.lastAudioSendTime < this.audioSendThrottle) {
      return;
    }

    // üîß NOVO: Detectar se h√° √°udio significativo
    const hasSignificantAudio = this.hasSignificantAudio(audioData);
    
    if (!hasSignificantAudio) {
      this.consecutiveSilenceFrames++;
      
      // Se temos muitos frames de sil√™ncio consecutivos, parar de enviar
      if (this.consecutiveSilenceFrames > this.maxSilenceFrames) {
        return;
      }
    } else {
      // Reset contador de sil√™ncio quando h√° √°udio
      this.consecutiveSilenceFrames = 0;
    }

    try {
      const base64Audio = this.arrayBufferToBase64(audioData.buffer as ArrayBuffer);
      
      this.sendEvent({
        type: 'input_audio_buffer.append',
        audio: base64Audio
      });
      
      this.lastAudioSendTime = now;
      
      // Log apenas ocasionalmente para evitar spam
      if (now % 1000 < this.audioSendThrottle) {
        console.log('üì§ [OPTIMIZED] Audio sent - silence frames:', this.consecutiveSilenceFrames);
      }
    } catch (error) {
      console.error('‚ùå [FIXED] Error sending audio:', error);
    }
  }

  // üîß NOVO: Detectar se h√° √°udio significativo
  private hasSignificantAudio(audioData: Int16Array): boolean {
    // Calcular RMS (Root Mean Square) para detectar n√≠vel de √°udio
    let rms = 0;
    for (let i = 0; i < audioData.length; i++) {
      const sample = audioData[i] / 32768.0; // Normalizar para -1 a 1
      rms += sample * sample;
    }
    rms = Math.sqrt(rms / audioData.length);
    
    // Retornar true se o n√≠vel est√° acima do threshold
    return rms > this.silenceThreshold;
  }

  // üîß NOVO: Detectar se h√° √°udio significativo no buffer acumulado
  private hasSignificantAudioInBuffer(): boolean {
    if (this.audioBuffer.length === 0) return false;
    
    let totalRms = 0;
    let totalSamples = 0;
    
    for (const chunk of this.audioBuffer) {
      for (let i = 0; i < chunk.length; i++) {
        totalRms += chunk[i] * chunk[i];
        totalSamples++;
      }
    }
    
    if (totalSamples === 0) return false;
    
    const rms = Math.sqrt(totalRms / totalSamples);
    return rms > this.silenceThreshold;
  }

  // üîß NOVO: Enviar buffer acumulado
  private flushAudioBuffer(): void {
    if (this.audioBuffer.length === 0) return;
    
    // Concatenar todos os chunks do buffer
    const totalSamples = this.audioBufferSize;
    const combinedBuffer = new Float32Array(totalSamples);
    
    let offset = 0;
    for (const chunk of this.audioBuffer) {
      combinedBuffer.set(chunk, offset);
      offset += chunk.length;
    }
    
    // Converter para Int16 e enviar
    const int16Array = new Int16Array(totalSamples);
    for (let i = 0; i < totalSamples; i++) {
      int16Array[i] = Math.max(-32768, Math.min(32767, combinedBuffer[i] * 32768));
    }
    
    this.sendAudioData(int16Array);
    
    // Limpar buffer
    this.audioBuffer = [];
    this.audioBufferSize = 0;
  }

  // üîÑ Utilit√°rios de convers√£o
  private arrayBufferToBase64(buffer: ArrayBuffer): string {
    const bytes = new Uint8Array(buffer);
    let binary = '';
    for (let i = 0; i < bytes.byteLength; i++) {
      binary += String.fromCharCode(bytes[i]);
    }
    return btoa(binary);
  }

  private base64ToArrayBuffer(base64: string): ArrayBuffer {
    const binaryString = atob(base64);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }
    return bytes.buffer;
  }

  // üîä NOVO: Sistema de reprodu√ß√£o de √°udio com fila (evita duplica√ß√£o)
  private playAudio(base64Audio: string): void {
    if (!this.audioContext || !this.audioGainNode) return;

    // Adicionar √† fila
    this.audioQueue.push(base64Audio);
    
    // Se n√£o estiver reproduzindo, iniciar reprodu√ß√£o
    if (!this.isPlayingAudio) {
      this.processAudioQueue();
    }
  }

  // üîä NOVO: Processar fila de √°udio sequencialmente
  private async processAudioQueue(): Promise<void> {
    if (this.audioQueue.length === 0 || this.isPlayingAudio) {
      return;
    }

    this.isPlayingAudio = true;

    while (this.audioQueue.length > 0) {
      const base64Audio = this.audioQueue.shift();
      if (!base64Audio) continue;

      try {
        await this.playAudioChunk(base64Audio);
        // üîß WORKAROUND: Pequeno delay entre chunks para evitar cortes
        await new Promise(resolve => setTimeout(resolve, 10));
      } catch (error) {
        console.error('‚ùå [FIXED] Error playing audio chunk:', error);
      }
    }

    // üîß WORKAROUND: Delay adicional ap√≥s processar toda a fila
    // para garantir que o √∫ltimo chunk seja reproduzido completamente
    await new Promise(resolve => setTimeout(resolve, 100));

    this.isPlayingAudio = false;
    console.log('üîä [QUEUE] Audio queue processing completed');
  }

  // üîä NOVO: Reproduzir um chunk de √°udio
  private playAudioChunk(base64Audio: string): Promise<void> {
    return new Promise((resolve, reject) => {
      if (!this.audioContext || !this.audioGainNode) {
        reject(new Error('Audio context not available'));
        return;
      }

      try {
        const audioBuffer = this.base64ToArrayBuffer(base64Audio);
        const audioData = new Int16Array(audioBuffer);
        
        // üîß NOVO: Processamento avan√ßado para eliminar picotamento e ru√≠do
        const floatArray = new Float32Array(audioData.length);
        
        // Converter Int16 para Float32 com normaliza√ß√£o melhorada
        for (let i = 0; i < audioData.length; i++) {
          floatArray[i] = audioData[i] / 32768.0;
        }
        
        // üîß NOVO: Filtro de ru√≠do avan√ßado - remover DC offset
        let dcOffset = 0;
        for (let i = 0; i < floatArray.length; i++) {
          dcOffset += floatArray[i];
        }
        dcOffset /= floatArray.length;
        
        for (let i = 0; i < floatArray.length; i++) {
          floatArray[i] -= dcOffset;
        }
        
        // üîß NOVO: Suaviza√ß√£o mais agressiva para eliminar picotamento
        const smoothedArray = new Float32Array(floatArray.length);
        const smoothingFactor = 0.3; // Suaviza√ß√£o mais forte
        
        smoothedArray[0] = floatArray[0];
        for (let i = 1; i < floatArray.length; i++) {
          smoothedArray[i] = smoothingFactor * floatArray[i] + (1 - smoothingFactor) * smoothedArray[i - 1];
        }
        
        // üîß NOVO: Fade-in/out mais longo para eliminar cliques
        const fadeLength = Math.min(512, Math.floor(smoothedArray.length * 0.02)); // 2% ou 512 samples
        
        // Fade-in suave no in√≠cio
        for (let i = 0; i < fadeLength; i++) {
          const fadeMultiplier = Math.sin((i / fadeLength) * Math.PI * 0.5); // Curva senoidal suave
          smoothedArray[i] *= fadeMultiplier;
        }
        
        // Fade-out suave no final
        for (let i = smoothedArray.length - fadeLength; i < smoothedArray.length; i++) {
          const fadeMultiplier = Math.sin(((smoothedArray.length - 1 - i) / fadeLength) * Math.PI * 0.5);
          smoothedArray[i] *= fadeMultiplier;
        }

        // üîß NOVO: Criar buffer com configura√ß√µes de alta qualidade
        const buffer = this.audioContext.createBuffer(1, smoothedArray.length, 24000);
        buffer.copyToChannel(smoothedArray, 0);

        // üõë IMPORTANTE: Parar √°udio anterior se existir
        if (this.currentAudioSource) {
          try {
            this.currentAudioSource.stop();
            this.currentAudioSource.disconnect();
          } catch (e) {
            // Ignorar erros de stop em sources j√° finalizados
          }
        }

        const source = this.audioContext.createBufferSource();
        source.buffer = buffer;
        
        // üîß NOVO: Cadeia de processamento anti-ru√≠do
        
        // 1. Filtro passa-alta para remover ru√≠do de baixa frequ√™ncia
        const highPassFilter = this.audioContext.createBiquadFilter();
        highPassFilter.type = 'highpass';
        highPassFilter.frequency.setValueAtTime(80, this.audioContext.currentTime); // Remove ru√≠do abaixo de 80Hz
        highPassFilter.Q.setValueAtTime(0.7, this.audioContext.currentTime);
        
        // 2. Filtro passa-baixa para remover ru√≠do de alta frequ√™ncia
        const lowPassFilter = this.audioContext.createBiquadFilter();
        lowPassFilter.type = 'lowpass';
        lowPassFilter.frequency.setValueAtTime(8000, this.audioContext.currentTime); // Reduzido para voz humana
        lowPassFilter.Q.setValueAtTime(0.5, this.audioContext.currentTime);
        
        // 3. Filtro notch para remover frequ√™ncias problem√°ticas
        const notchFilter = this.audioContext.createBiquadFilter();
        notchFilter.type = 'notch';
        notchFilter.frequency.setValueAtTime(60, this.audioContext.currentTime); // Remove hum de 60Hz
        notchFilter.Q.setValueAtTime(10, this.audioContext.currentTime);
        
        // 4. Compressor mais suave para evitar distor√ß√£o
        const compressor = this.audioContext.createDynamicsCompressor();
        compressor.threshold.setValueAtTime(-18, this.audioContext.currentTime); // Menos agressivo
        compressor.knee.setValueAtTime(20, this.audioContext.currentTime);
        compressor.ratio.setValueAtTime(6, this.audioContext.currentTime); // Menos compress√£o
        compressor.attack.setValueAtTime(0.01, this.audioContext.currentTime);
        compressor.release.setValueAtTime(0.1, this.audioContext.currentTime);
        
        // üîß NOVO: Cadeia de processamento otimizada
        source.connect(highPassFilter);
        highPassFilter.connect(notchFilter);
        notchFilter.connect(lowPassFilter);
        lowPassFilter.connect(compressor);
        compressor.connect(this.audioGainNode);
        
        // Armazenar refer√™ncia para controle
        this.currentAudioSource = source;

        source.onended = () => {
          this.currentAudioSource = null;
          resolve();
        };

        // üîß NOVO: Iniciar com timing mais preciso para evitar glitches
        const startTime = this.audioContext.currentTime + 0.005; // Delay menor mas preciso
        source.start(startTime);

      } catch (error) {
        console.error('‚ùå [FIXED] Error playing audio chunk:', error);
        reject(error);
      }
    });
  }

  // üõë NOVO: Parar reprodu√ß√£o de √°udio
  private stopCurrentAudio(): void {
    if (this.currentAudioSource) {
      try {
        this.currentAudioSource.stop();
        this.currentAudioSource.disconnect();
      } catch (e) {
        // Ignorar erros
      }
      this.currentAudioSource = null;
    }
    
    // Limpar fila
    this.audioQueue = [];
    this.isPlayingAudio = false;
  }

  // üéß Sistema de eventos
  on(eventType: string, callback: Function): void {
    if (!this.eventListeners.has(eventType)) {
      this.eventListeners.set(eventType, []);
    }
    this.eventListeners.get(eventType)!.push(callback);
  }

  off(eventType: string, callback: Function): void {
    const listeners = this.eventListeners.get(eventType);
    if (listeners) {
      const index = listeners.indexOf(callback);
      if (index > -1) {
        listeners.splice(index, 1);
      }
    }
  }

  private emit(eventType: string, data?: any): void {
    const listeners = this.eventListeners.get(eventType);
    if (listeners) {
      listeners.forEach(callback => {
        try {
          callback(data);
        } catch (error) {
          console.error('‚ùå [FIXED] Error in event listener:', error);
        }
      });
    }
  }

  // üé§ Fun√ß√µes de controle
  createResponse(): void {
    this.sendEvent({
      type: 'response.create',
      response: {
        modalities: ['text', 'audio'],
        instructions: `Respond naturally as Charlotte, adapting to the user's ${this.config.userLevel} level.`
      }
    });
  }

  // üîß NOVO: Interrup√ß√£o inteligente com verifica√ß√£o de estado
  interruptResponse(): void {
    console.log('üõë [INTERRUPT DEBUG] interruptResponse() called');
    console.log('üõë [INTERRUPT DEBUG] Service state:', {
      isConnected: this.isConnected,
      wsReadyState: this.ws?.readyState,
      isPlayingAudio: this.isPlayingAudio,
      currentAudioSource: !!this.currentAudioSource,
      isCharlotteSpeaking: this.isCharlotteSpeaking,
      hasActiveResponse: this.hasActiveResponse
    });
    
    // üõë NOVO: S√≥ tentar cancelar se h√° resposta ativa
    if (!this.hasActiveResponse && !this.isCharlotteSpeaking) {
      console.log('üõë [INTERRUPT DEBUG] No active response to cancel - skipping API call');
      // Ainda limpar buffers locais por seguran√ßa
      this.stopCurrentAudio();
      return;
    }
    
    // üõë NOVO: Parar √°udio atual antes de cancelar resposta
    this.stopCurrentAudio();
    console.log('üõë [INTERRUPT DEBUG] Audio stopped');
    
    // üîß NOVO: Ajustar temporariamente o VAD para ser menos sens√≠vel ap√≥s interrup√ß√£o
    this.adjustVADSensitivity('post_interruption');
    console.log('üõë [INTERRUPT DEBUG] VAD adjusted for post-interruption');
    
    // üõë NOVO: S√≥ enviar cancel se h√° resposta ativa
    if (this.hasActiveResponse) {
    this.sendEvent({
      type: 'response.cancel'
    });
    console.log('üõë [INTERRUPT DEBUG] response.cancel sent to API');
    }
    
    // üõë NOVO: Resetar estados
    this.hasActiveResponse = false;
    this.isCharlotteSpeaking = false;
    
    // üîß NOVO: Restaurar VAD ap√≥s um tempo
    setTimeout(() => {
      console.log('üõë [INTERRUPT DEBUG] Restoring normal VAD sensitivity');
      this.adjustVADSensitivity('normal');
    }, 2000);
  }

  // üîß ATUALIZADO: Ajustar sensibilidade do VAD dinamicamente (simplificado)
  private adjustVADSensitivity(mode: 'normal' | 'post_interruption' | 'sensitive' | 'short_words'): void {
    let vadConfig;
    
    console.log(`üîß [VAD] Adjusting VAD mode to: ${mode} (User level: ${this.config.userLevel})`);
    
    switch (mode) {
      case 'short_words':
        // Para palavras curtas, usar VAD mais sens√≠vel
        vadConfig = {
          type: 'server_vad',
          threshold: 0.2, // Muito sens√≠vel para capturar palavras curtas
          prefix_padding_ms: 150,
          silence_duration_ms: 300,
          create_response: true
        };
        console.log('üîß [VAD] Using sensitive VAD for short words detection');
        break;
        
      case 'post_interruption':
        // Ap√≥s interrup√ß√£o, VAD um pouco menos sens√≠vel temporariamente
        vadConfig = {
          type: 'server_vad',
          threshold: 0.6,
          prefix_padding_ms: 200,
          silence_duration_ms: 500,
          create_response: true
        };
        console.log('üîß [VAD] Using post-interruption VAD (temporarily less sensitive)');
        break;
        
      case 'sensitive':
        // VAD sens√≠vel geral
        vadConfig = {
          type: 'server_vad',
          threshold: 0.3,
          prefix_padding_ms: 150,
          silence_duration_ms: 350,
          create_response: true
        };
        console.log('üîß [VAD] Using general sensitive VAD');
        break;
        
      default: // normal
        // VAD ultra-responsivo baseado no n√≠vel do usu√°rio (como ChatGPT)
        vadConfig = this.getVADConfigForUserLevel();
        console.log(`üîß [VAD] Using ChatGPT-like VAD (${vadConfig.threshold} threshold) for ${this.config.userLevel} level`);
    }
    
    console.log(`üîß [VAD] Sending session update with VAD config:`, vadConfig);
    
    this.sendEvent({
      type: 'session.update',
      session: {
        turn_detection: vadConfig
      }
    });
  }

  // üîß ATUALIZADO: Detectar palavras curtas e ajustar VAD (simplificado - sem comandos espec√≠ficos)
  private handleShortWordDetection(transcript: string): void {
    // Lista expandida de palavras curtas comuns em ingl√™s
    const shortWords = [
      // Respostas b√°sicas
      'yes', 'no', 'ok', 'okay', 'yeah', 'yep', 'nah', 'nope',
      // Cumprimentos
      'hi', 'hey', 'hello', 'bye', 'goodbye',
      // Confirma√ß√µes
      'sure', 'right', 'correct', 'wrong', 'true', 'false',
      // Avalia√ß√µes
      'good', 'bad', 'great', 'nice', 'cool', 'wow',
      // N√∫meros b√°sicos
      'one', 'two', 'three', 'four', 'five',
      // Palavras de hesita√ß√£o (importantes para estudantes)
      'um', 'uh', 'er', 'hmm', 'well',
      // Comandos diversos (mantidos para refer√™ncia)
      'stop', 'wait', 'pause', 'enough', 'done', 'finish'
    ];
    
    const words = transcript.toLowerCase().trim().split(/\s+/);
    const isShortResponse = words.length <= 2;
    const containsShortWord = words.some(word => shortWords.includes(word.replace(/[.,!?]/g, '')));
    
    // üìù NOTA: N√£o precisamos mais de l√≥gica especial para comandos de parada
    // porque agora qualquer fala interrompe imediatamente via speech_started
    
    if (isShortResponse && containsShortWord) {
      console.log(`üîß [VAD] Short word/phrase detected: "${transcript}" - maintaining high sensitivity`);
      
      // Manter VAD sens√≠vel para palavras curtas
      this.adjustVADSensitivity('short_words');
      
      // Restaurar ap√≥s tempo baseado no n√≠vel do usu√°rio
      const restoreDelay = this.getRestoreDelayForUserLevel();
      setTimeout(() => {
        console.log(`üîß [VAD] Restoring normal VAD after ${restoreDelay}ms`);
        this.adjustVADSensitivity('normal');
      }, restoreDelay);
    }
  }

  // üîß NOVO: Tempo de restaura√ß√£o baseado no n√≠vel do usu√°rio
  private getRestoreDelayForUserLevel(): number {
    const delays = {
      'Novice': 8000,     // 8 segundos - mais tempo para iniciantes pensarem
      'Inter': 6000,      // 6 segundos - tempo moderado
      'Advanced': 4000    // 4 segundos - menos tempo para avan√ßados
    };
    
    const delay = delays[this.config.userLevel] || delays['Inter'];
    console.log(`‚è±Ô∏è [VAD] Restore delay for ${this.config.userLevel} level: ${delay}ms`);
    
    return delay;
  }

  sendFunctionResult(callId: string, output: string): void {
    this.sendEvent({
      type: 'conversation.item.create',
      item: {
        type: 'function_call_output',
        call_id: callId,
        output: output
      }
    });
  }

  sendTextMessage(text: string): void {
    this.sendEvent({
      type: 'conversation.item.create',
      item: {
        type: 'message',
        role: 'user',
        content: [{ type: 'input_text', text: text }]
      }
    });
    this.createResponse();
  }

  commitAudio(): void {
    this.sendEvent({
      type: 'input_audio_buffer.commit'
    });
    this.createResponse();
  }

  clearAudioBuffer(): void {
    this.sendEvent({
      type: 'input_audio_buffer.clear'
    });
  }

  // üõë NOVO: Limpeza emergencial de todos os buffers para comandos de parada
  private emergencyClearAllBuffers(): void {
    console.log('üõë [EMERGENCY] Clearing all audio buffers immediately');
    
    // Limpar buffer da API
    this.sendEvent({
      type: 'input_audio_buffer.clear'
    });
    
    // Limpar buffer interno
    this.audioBuffer = [];
    this.audioBufferSize = 0;
    this.consecutiveSilenceFrames = 0;
    
    // Limpar fila de reprodu√ß√£o
    this.audioQueue = [];
    
    // Parar √°udio atual
    this.stopCurrentAudio();
    
    // Reset throttling para permitir comandos imediatos
    this.lastAudioSendTime = 0;
    
    // üõë NOVO: Limpar delta transcript acumulado
    this.currentTranscriptDelta = '';
    
    // üõë NOVO: Resetar estados de fala
    this.isCharlotteSpeaking = false;
    this.hasActiveResponse = false;
    
    console.log('üõë [EMERGENCY] All buffers cleared successfully');
  }

  // üîå Limpeza e desconex√£o
  disconnect(): void {
    console.log('üîå [FIXED] Disconnecting...');
    
    // üîß NOVO: Parar grava√ß√£o
    this.isRecording = false;
    
    // üõë Parar √°udio atual
    this.stopCurrentAudio();
    
    // üîå Fechar WebSocket
    if (this.ws) {
      this.ws.close();
      this.ws = null;
    }
    
    // üé§ Limpar recursos de √°udio
    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach(track => track.stop());
      this.mediaStream = null;
    }
    
    // üîß NOVO: Limpar recursos de processamento avan√ßado
    if (this.microphoneSource) {
      this.microphoneSource.disconnect();
      this.microphoneSource = null;
    }
    
    if (this.audioProcessor) {
      this.audioProcessor.disconnect();
      this.audioProcessor = null;
    }
    
    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }
    
    // üîß NOVO: Limpar controles de otimiza√ß√£o de √°udio
    this.audioBuffer = [];
    this.audioBufferSize = 0;
    this.consecutiveSilenceFrames = 0;
    this.lastAudioSendTime = 0;
    
    // üõë NOVO: Limpar delta transcript acumulado
    this.currentTranscriptDelta = '';
    
    // üõë NOVO: Resetar estados de fala
    this.isCharlotteSpeaking = false;
    this.hasActiveResponse = false;
    
    this.isConnected = false;
    this.sessionId = null;
    this.audioQueue = [];
    this.isPlayingAudio = false;
    
    console.log('‚úÖ [FIXED] Disconnected successfully');
  }

  // üìä Getters
  get connected(): boolean {
    return this.isConnected && this.ws?.readyState === WebSocket.OPEN;
  }

  get session(): string | null {
    return this.sessionId;
  }

  // üîç M√©todo de diagn√≥stico
  async diagnose(): Promise<any> {
    console.log('üîç [FIXED] Running connection diagnosis...');
    
    const diagnosis = {
      timestamp: new Date().toISOString(),
      apiKeyConfigured: !!this.config.apiKey,
      apiKeyLength: this.config.apiKey?.length || 0,
      websocketSupport: typeof WebSocket !== 'undefined',
      audioSupport: !!navigator.mediaDevices?.getUserMedia,
      connectionState: this.ws?.readyState,
      isConnected: this.isConnected,
      sessionId: this.sessionId,
      reconnectAttempts: this.reconnectAttempts
    };

    console.log('üìä [FIXED] Diagnosis:', diagnosis);
    return diagnosis;
  }

  // üé§ Configurar captura de √°udio do microfone
  private async setupMicrophone(): Promise<void> {
    try {
      console.log('üé§ [FIXED] Setting up microphone...');
      
      // üîß NOVO: Primeiro tentar configura√ß√µes b√°sicas se as avan√ßadas falharem
      let constraints: MediaStreamConstraints;
      
      try {
        // Tentar configura√ß√µes avan√ßadas primeiro
        constraints = {
        audio: {
          sampleRate: 24000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          // üîß NOVO: Configura√ß√µes avan√ßadas para reduzir ru√≠do
          googEchoCancellation: true,
          googAutoGainControl: true,
          googNoiseSuppression: true,
          googHighpassFilter: true,
          googTypingNoiseDetection: true,
          googAudioMirroring: false,
          // üîß NOVO: Configura√ß√µes de qualidade premium
          latency: 0.01, // Lat√™ncia m√≠nima
          volume: 0.8,   // Volume controlado
        } as any
      };

        console.log('üé§ [FIXED] Trying advanced microphone configuration...');
      this.mediaStream = await navigator.mediaDevices.getUserMedia(constraints);
        
      } catch (advancedError) {
        console.log('‚ö†Ô∏è [FIXED] Advanced config failed, trying basic configuration...');
        
        // Fallback para configura√ß√µes b√°sicas
        constraints = {
          audio: {
            sampleRate: 24000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          }
        };

        try {
          this.mediaStream = await navigator.mediaDevices.getUserMedia(constraints);
        } catch (basicError) {
          console.log('‚ö†Ô∏è [FIXED] Basic config failed, trying minimal configuration...');
          
          // √öltimo recurso - configura√ß√£o m√≠nima
          constraints = {
            audio: true
          };
          
          this.mediaStream = await navigator.mediaDevices.getUserMedia(constraints);
        }
      }
      
      console.log('‚úÖ [FIXED] Microphone access granted');
      
      // üîß NOVO: Configurar processamento avan√ßado do microfone
      this.microphoneSource = this.audioContext!.createMediaStreamSource(this.mediaStream);
      
      // üîß NOVO: Cadeia de filtros para o microfone (reduzir ru√≠do de entrada)
      
      // 1. Filtro passa-alta para remover ru√≠do de baixa frequ√™ncia do mic
      const micHighPass = this.audioContext!.createBiquadFilter();
      micHighPass.type = 'highpass';
      micHighPass.frequency.setValueAtTime(100, this.audioContext!.currentTime); // Remove ru√≠do abaixo de 100Hz
      micHighPass.Q.setValueAtTime(0.5, this.audioContext!.currentTime);
      
      // 2. Filtro passa-baixa para limitar frequ√™ncias do mic
      const micLowPass = this.audioContext!.createBiquadFilter();
      micLowPass.type = 'lowpass';
      micLowPass.frequency.setValueAtTime(8000, this.audioContext!.currentTime); // Limita a 8kHz
      micLowPass.Q.setValueAtTime(0.7, this.audioContext!.currentTime);
      
      // 3. Compressor suave para o microfone
      const micCompressor = this.audioContext!.createDynamicsCompressor();
      micCompressor.threshold.setValueAtTime(-20, this.audioContext!.currentTime);
      micCompressor.knee.setValueAtTime(15, this.audioContext!.currentTime);
      micCompressor.ratio.setValueAtTime(4, this.audioContext!.currentTime);
      micCompressor.attack.setValueAtTime(0.005, this.audioContext!.currentTime);
      micCompressor.release.setValueAtTime(0.05, this.audioContext!.currentTime);
      
      // üîß NOVO: Conectar cadeia de processamento do microfone
      this.microphoneSource.connect(micHighPass);
      micHighPass.connect(micLowPass);
      micLowPass.connect(micCompressor);
      
      // Conectar ao processador de √°udio
      this.audioProcessor = this.audioContext!.createScriptProcessor(1024, 1, 1); // Buffer menor para menos lat√™ncia
      micCompressor.connect(this.audioProcessor);
      this.audioProcessor.connect(this.audioContext!.destination);

      // üîß NOVO: Processamento de √°udio melhorado para envio
      this.audioProcessor.onaudioprocess = (event: AudioProcessingEvent) => {
        if (!this.isRecording) return;

        const inputBuffer = event.inputBuffer;
        const inputData = inputBuffer.getChannelData(0);
        
        // üîß NOVO: Aplicar gate de ru√≠do mais eficiente
        const noiseGate = -45; // dB - mais restritivo
        let rms = 0;
        for (let i = 0; i < inputData.length; i++) {
          rms += inputData[i] * inputData[i];
        }
        rms = Math.sqrt(rms / inputData.length);
        const dbLevel = 20 * Math.log10(rms);
        
        // Se o n√≠vel est√° abaixo do gate, silenciar completamente
        if (dbLevel < noiseGate) {
          inputData.fill(0);
        }
        
        // üîß NOVO: Normaliza√ß√£o suave para evitar clipping
        const maxAmplitude = Math.max(...Array.from(inputData, (x: number) => Math.abs(x)));
        if (maxAmplitude > 0.95) {
          const normalizationFactor = 0.9 / maxAmplitude;
          for (let i = 0; i < inputData.length; i++) {
            inputData[i] *= normalizationFactor;
          }
        }

        // üîß NOVO: Buffering inteligente - acumular dados antes de enviar
        this.audioBuffer.push(new Float32Array(inputData));
        this.audioBufferSize += inputData.length;
        
        // Enviar apenas quando o buffer estiver cheio ou houver √°udio significativo
        if (this.audioBufferSize >= this.maxBufferSize || this.hasSignificantAudioInBuffer()) {
          this.flushAudioBuffer();
        }
      };

      console.log('‚úÖ [FIXED] Microphone setup complete with advanced noise reduction');
      
    } catch (error: any) {
      console.error('‚ùå [FIXED] Error setting up microphone:', error);
      
      // üîß NOVO: Melhor tratamento de erros espec√≠ficos
      if (error.name === 'NotFoundError') {
        throw new Error('Microfone n√£o encontrado. Verifique se h√° um microfone conectado ao dispositivo.');
      } else if (error.name === 'NotAllowedError') {
        throw new Error('Permiss√£o negada para acessar o microfone. Clique no √≠cone do microfone na barra de endere√ßos e permita o acesso.');
      } else if (error.name === 'NotReadableError') {
        throw new Error('Microfone est√° sendo usado por outro aplicativo. Feche outros programas que possam estar usando o microfone.');
      } else if (error.name === 'OverconstrainedError') {
        throw new Error('Configura√ß√µes do microfone n√£o suportadas. Tentando com configura√ß√µes b√°sicas...');
      } else {
        throw new Error(`Erro ao configurar microfone: ${error.message}`);
      }
    }
  }

  // üîß NOVO: Parar grava√ß√£o
  private stopRecording(): void {
    if (this.isRecording) {
      this.isRecording = false;
      
      // üîß NOVO: For√ßar envio do buffer restante antes de parar
      this.forceFlushAudioBuffer();
      
      console.log('üé§ [OPTIMIZED] Recording stopped and buffer flushed');
    }
  }

  // üîß NOVO: Verificar se o usu√°rio ainda est√° falando
  private isUserStillSpeaking(): boolean {
    // Verificar se o usu√°rio est√° atualmente falando e h√° tempo suficiente
    const speechDuration = Date.now() - this.userSpeechStartTime;
    const minSpeechDuration = 200; // Reduzido de 400ms para 200ms - mais responsivo para palavras como "Charlotte"
    
    const stillSpeaking = this.isUserCurrentlySpeaking && speechDuration >= minSpeechDuration;
    
    console.log(`üé§ [SPEECH CHECK] User speaking: ${this.isUserCurrentlySpeaking}, Duration: ${speechDuration}ms, Valid: ${stillSpeaking}`);
    
    return stillSpeaking;
  }
}

// üöÄ Fun√ß√£o de conveni√™ncia para uso simplificado
export async function createRealtimeConnection(config: RealtimeConfig): Promise<OpenAIRealtimeService> {
  const service = new OpenAIRealtimeService(config);
  
  try {
    await service.connect();
    await service.initializeAudio();
    return service;
  } catch (error) {
    console.error('‚ùå [FIXED] Failed to create Realtime connection:', error);
    throw error;
  }
}

// üîß Fun√ß√£o para verificar se a conta tem acesso √† Realtime API
export async function checkRealtimeAccess(): Promise<{
  hasAccess: boolean;
  models: string[];
  error?: string;
}> {
  try {
    const response = await fetch('/api/realtime-token', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        userLevel: 'Intermediate',
        userName: 'Access Check',
        debug: true
      })
    });

    if (!response.ok) {
      return {
        hasAccess: false,
        models: [],
        error: `Token API failed: ${response.status}`
      };
    }

    const data = await response.json();
    
    if (!data.success) {
      return {
        hasAccess: false,
        models: [],
        error: data.error || 'Unknown error'
      };
    }

    // Se chegou at√© aqui, provavelmente tem acesso
    return {
      hasAccess: true,
      models: ['gpt-4o-mini-realtime-preview-2024-12-17'],
      error: undefined
    };

  } catch (error: any) {
    return {
      hasAccess: false,
      models: [],
      error: error.message
    };
  }
}

export default OpenAIRealtimeService;